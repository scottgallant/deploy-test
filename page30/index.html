<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
  <head>
    <title>paperplanes. mathias meyer.</title>
    <meta name="robots" content="index,follow"/>
    <meta name="mssmarttagspreventparsing" content="true"/>
    <link rel="shortcut icon" href="/images/favicon.gif" type="image/gif" />
    <link rel="icon" href="/images/favicon.gif" type="image/gif" />
    <meta http-equiv="content-type" content="text/html; charset=utf-8"/>
  	<meta name="author" content="Mathias Meyer"/>
    <meta name="dc.title" content="paperplanes. mathias meyer."/>
  	<link rel="start" href="http://www.paperplanes.de" title="paperplanes"/>
    
    <link href="http://www.paperplanes.de/rss.xml" rel="alternate" title="Primary Feed" type="application/rss+xml" />
    <link href="/stylesheets/screen.css" media="screen" rel="Stylesheet" title="paperplanes" type="text/css" />
    <link rel="stylesheet" type="text/css" href="/stylesheets/mobile.css" media="handheld, only screen and (max-device-width: 960px)" />
  </head>
  
  <body id="www-paperplanes-de">
    <div id="head">
      <div id="header-content">
        <a href="/">
          <img src="/images/paperplane.png" id="paperplane">
        </a>
        <div id="about">
          <h1 class="default">Hi, I'm Mathias Meyer, nice to meet you!</h1>
          <h1 class="mobile">Hi, I'm <a href="https://twitter.com/roidrage">Mathias Meyer</a>, I'm the CEO at <a href="https://travis-ci.com">Travis CI</a></h1>
          <p style="color: white" class="about-sub-title default">
            I'm the CEO at <a href="http://travis-ci.com">Travis CI</a>. I like coffee, <a href="https://twitter.com/roidrage">Twitter</a> and <a href="mailto:meyer@paperplanes.de">email</a>.
          </p>
        </div>
      </div>
    </div>

    <div id="box">
      <div id="content">
        <div id="articles">

  <article>
    <div class="item">
      <div class="item_details">
        <h3><a href="/2010/9/3/why_i_love_and_hate_distributed_systems.html">Why I Love and Hate Distributed Systems</a></h3>
        <h4><a href="/2010/9/3/why_i_love_and_hate_distributed_systems.html" title="Why I Love and Hate Distributed Systems">03 September 2010</a> by <a href="http://twitter.com/roidrage">Mathias Meyer</a></h4>
      </div>
      <div class="item_content">
        <p>Let me go ahead and say it: I love distributed systems. Why? Simply because they bend my brain. Yesterday I tweeted
&quot;Distributed databases are my happy place.&quot; One response I got was along the lines of: &quot;then you&#39;re probably not running
a distributed database in production.&quot; Busted! But does it matter? We all love distributed stuff, we love thinking about
scaling. They seem like problems everyone wants to have and solve.</p>

<p>But the truth is, I don&#39;t, and I can assure you, you don&#39;t want to either, sometimes I doubt my brain is even capable
properly solving these problems, but that doesn&#39;t prevent me from trying. I prefer to work on as small a scale as
possible, you could even say I hate distributed systems. Scaling and distribution is a problem most of us don&#39;t have,
and are probably better of not having.</p>

<p>Truth be told, I&#39;m not highly interested in running highly distributed systems in production, quite the opposite. I prefer
maxing out what I have as far up as possible. Sometimes I do take the plunge and just try something new in production,
but I&#39;m happy prepared to replace it with something different, even something simpler, if that seems like the better
option in the end. Everyone should experiment at some point, but not all the time.</p>

<p>But why then do I love distributed systems? Simply because they make me think about how they could be put to use,
what algorithms and the problems involved are, and what implications they would have on a production system, both from
an operations and developer perspective. That&#39;s where the value is for me, it allows me to simply make informed
decisions when the time comes.</p>

<p>Take Riak, for example, on which <a href="http://riak-rugb.heroku.com">I gave a shortish talk</a> at yesterday&#39;s meet-up of the
local Ruby brigade. Riak&#39;s distribution model is based on Amazon&#39;s Dynamo implementation, with some neat features
sprinkled on top. Riak is built by a bunch of really, really smart guys at <a href="http://basho.com">Basho</a>, whose work I have
nothing but respect for, but who also are sane and open enough to tell people when their database may or may not be a
good fit (something a certain other database is severly lacking).</p>

<p>Riak is exciting for me because it was the first database that really made me dive into Amazon&#39;s Dynamo, and once I
started grokking it, it blew my mind. If you haven&#39;t read it, <a href="http://www.allthingsdistributed.com/2007/10/amazons_dynamo.html">please
do</a>. It blew my mind simply because it introduced me to
a whole new thinking, to heavily distributed storage, with all the potential hot spots, downsides and business use cases
for specifics parts of it thrown in. The same is true for <a href="http://labs.google.com/papers/bigtable.html">Google&#39;s BigTable</a>.
The technologies involved with both are true mind-benders.</p>

<p>And there&#39;s my bottom line. Distributed systems aren&#39;t necessarily awesome just because they allow scaling to infinite
heights (exaggeration intended), but because they broaden your personal horizon. It&#39;s like learning new programming
languages. It&#39;s about getting new ideas in your head, ideas outside of your everyday working realm. Ideas you can maybe
even take back to what you&#39;re working on and start applying them where it makes sense, and only if it makes sense.
Learning about distributed systems is not just about learning how to use them, but when. Knowing is half the battle.</p>

<p>While you&#39;re at it, check out Evan Weaver&#39;s <a href="http://blog.evanweaver.com/articles/2009/05/04/distributed-systems-primer/">&quot;Distributed Systems
Primer&quot;</a>, a collection of papers on
distributed systems, or the <a href="http://nosqlsummer.org/papers">papers collection over at NoSQL Summer</a>. Get ready to have
your mind blown in whole new ways. Say what you will, that stuff is just fascinating. It appeals to the distributed
database lover in me.</p>

      </div>
      <div class="item_meta">
        <span class="item_tags">
          Tags: 
          
        </span>
      </div>
    </div>
  </article>

  <article>
    <div class="item">
      <div class="item_details">
        <h3><a href="/2010/8/12/inconvenient_caveat_about_mongodbs_replica_sets.html">An Inconvenient Caveat about MongoDB's Replica Sets (updated)</a></h3>
        <h4><a href="/2010/8/12/inconvenient_caveat_about_mongodbs_replica_sets.html" title="An Inconvenient Caveat about MongoDB's Replica Sets (updated)">12 August 2010</a> by <a href="http://twitter.com/roidrage">Mathias Meyer</a></h4>
      </div>
      <div class="item_content">
        <p>Update: Read the comments and below. The issue is not as bad as it used to be in the documentation and the original
design, thankfully.</p>

<p>A lot has happened since I&#39;ve first <a href="/2010/2/25/notes_on_mongodb.html">written about MongoDB back in
February</a>. Replica Pairs are going to be deprecated, being
replaced by Replica Sets, a working Auto-Sharding implementation, including rebalancing shards, and lots more, all
neatly wrapped into the <a href="http://blog.mongodb.org/post/908172564/mongodb-1-6-released">1.6 release</a>.</p>

<p>The initial draft on how they&#39;d turn out sounded good, but something struck me as odd, and it is once again one of these
things that tend to be overlooked in all the excitement about the new features. Before we dive any deeper, make sure
you&#39;ve read the <a href="http://www.mongodb.org/display/DOCS/Replica+Sets">documentation</a>, or check out this rather short
<a href="http://www.coffeepowered.net/2010/08/06/setting-up-replica-sets-with-mongodb-1-6/">introduction on setting up a Replica
Set</a>, I won&#39;t go into much detail on
Replica Sets in general, I just want to point out one major issue I&#39;ve found with them. Part of the <a href="http://www.mongodb.org/display/DOCS/Replica+Set+Internals">documentation sheds
some light on the inner workings of Replica Sets</a>. It&#39;s not
exhaustive, but to me more interesting than the rest of the documentation.</p>

<p>One part struck me as odd, the paragraph on <a href="http://www.mongodb.org/display/DOCS/Replica+Set+Internals#ReplicaSetInternals-Resync%28ConnectingtoaNewPrimary%29">resyncing data from a new
primary</a>
(as in master). It&#39;s two parts actually, but they pretty much describe the same caveat:</p>

<blockquote>
<p>When a secondary connects to a new primary, it must resynchronize its position. It is possible the secondary has
operations that were never committed at the primary. In this case, we roll those operations back.</p>
</blockquote>

<p>Also:</p>

<blockquote>
<p>When we become primary, we assume we have the latest data. Any data newer than the new primary&#39;s will be discarded.</p>
</blockquote>

<p>Did you notice something? MongoDB rolls operations back that were never committed to the primary, discarding the updated
data, which is just a fancy term for <a href="http://www.mongodb.org/display/DOCS/Replica+Set+Internals#ReplicaSetInternals-AssumptionofPrimary">silently deleting data without further
notice</a>. Imagine a
situation where you just threw a bunch of new or updated data at your current master, and the data has not yet fully
replicated to all slaves, when suddenly your master crashes. According to <a href="http://www.mongodb.org/display/DOCS/Replica+Set+Internals#ReplicaSetInternals-PickingPrimary">the
protocol</a> the node with
the most recent opslog entries takes over the primary&#39;s role automatically.</p>

<p>When the old master comes back up, it needs to resynchronize the changes from the current master, before it can play any
role in the set again, no matter if it becomes the new primary, or sticks to being a secondary, leaving the new master
in place. During that resync it discards data that has not been synchronized to the new master yet. If the opslog on the
new master was behind a couple of dozen entries before the old one went down, all that data is lost. I repeat: lost.
Think about that.</p>

<p><a href="http://www.mongodb.org/display/DOCS/Replica+Set+Internals#ReplicaSetInternals-IncreasingDurability">There&#39;s ways to reduce the
pain</a>, and I
appreciate that they&#39;re mentioned appropriately in the documentation. You can tell MongoDB to consider a write
successful when it replicated to a certain number of secondaries. But you have to wait until that happened, <a href="http://www.mongodb.org/display/DOCS/Verifying+Propagation+of+Writes+with+getLastError">polling
<code>getLastError()</code> for the state of the last
operation</a>. Or you could set
<code>maxLag</code> accordingly, so that the master will fail or block a write until the secondaries catch up with the replication,
though I couldn&#39;t for the life of me figure out (using the Googles) where and how to set it.</p>

<p>But I don&#39;t approve of this behavior as a default, and the fact that you need to go through the internals to find out
about it. <a href="http://www.mongodb.org/display/DOCS/Sharding+and+Failover">Everything else suggests</a> that there&#39;s no point of
failure in a MongoDB setup using sharding and Replica Sets, even comparing it to the Dynamo way of guaranteeing
consistency, which it simply isn&#39;t when the client has to poll for a successful write.</p>

<p>It&#39;s one of those things that make me reconsider my (already improved) opinions on MongoDB all over again, just when I
started to warm up with it. Yes, it&#39;s wicked fast, but I simply disagree with their take on durability and consistency.
The tradeoff (as in: losing data) is simply too big for me. You could argue that these situations will be quite rare,
and I would not disagree with you, but I&#39;m not fond of potentially losing data when they do happen. If this works for
you, cool! Just thought you should know.</p>

<p><em>Update</em>: There&#39;s been some helpful comments by the MongoDB folks, and there&#39;s good news. Data is not silently discarded
in 1.6 anymore, apparently it&#39;s stored in some flat file, fixed with <a href="http://jira.mongodb.org/browse/SERVER-1512">this
issue</a>, though it&#39;s hard for me to say from the commits what exactly
happens. The documentation does not at all reflect these changes, but improvements are on the way. I&#39;m still not happy
about some of the design decision, but they&#39;re rooted in the way MongoDB currently works, and changing that is unlikely
to happen, but at least losing data doesn&#39;t seem to be an option anymore. If making a bit of a fool out of myself helped
to improve on the documentation front, so be it. I can live with that.</p>

      </div>
      <div class="item_meta">
        <span class="item_tags">
          Tags: 
          
        </span>
      </div>
    </div>
  </article>

  <article>
    <div class="item">
      <div class="item_details">
        <h3><a href="/2010/7/26/10_annoying_things_about_couchdb.html">10 Annoying Things About CouchDB</a></h3>
        <h4><a href="/2010/7/26/10_annoying_things_about_couchdb.html" title="10 Annoying Things About CouchDB">26 July 2010</a> by <a href="http://twitter.com/roidrage">Mathias Meyer</a></h4>
      </div>
      <div class="item_content">
        <p>Hi, I&#39;m Mathias, and I&#39;m a <a href="http://couchdb.apache.org/">CouchDB</a> user. I&#39;ve been using it for almost a year now, and we
have a <a href="http://scalarium.com">project using it in production</a>, with a side of Redis. I think it&#39;s an awesome database,
some of its features are simply unrivaled. <a href="http://wiki.apache.org/couchdb/Replication">Offline replication</a>,
<a href="http://couchapp.org/">CouchApps</a>, to name a few. CouchDB just hit version 1.0.  It&#39;s been a long time coming, with
CouchDB having probably one of the longest histories in the non-relational database space. I&#39;ve heard about it first
back in September 2008, when Jan Lehnardt talked about it at a local co-working space.  I still blame him for getting me
all excited about this whole NoSQL thing. Fun fact: I bookmarked the CouchDB website back in February 2008.</p>

<p>The features being added to it with every release are nothing short of exciting. CouchDB 0.11 got <a href="http://blog.couch.io/post/446015664/whats-new-in-apache-couchdb-0-11-part-two-views">filtered
replication</a>, support for <a href="http://blog.couch.io/post/443028592/whats-new-in-apache-couchdb-0-11-part-one-nice-urls">URL
rewriting and vhosts</a>, amongst
other things. But there&#39;s still some things that annoy me, that somewhat bug me in my daily work with it.</p>

<p>The following things are not incredible pet-peeves I have with CouchDB. I think CouchDB is pretty awesome, and I really
like using it. However, it doesn&#39;t come without the occasional odditity that will leave you scratching your head. These
probably aren&#39;t the only things to be aware of, they&#39;re just the most annoying to me. Your mileage may vary. They may or
may not be annoying to you, but they&#39;re things that are good to know working with CouchDB. Whether CouchDB should or
should not have what I&#39;m listing here is a whole different story. It&#39;s my wishlist of improvements, if you will.</p>

<p>It&#39;s also stuff you&#39;re buying into when you move off the beaten path of relational databases. As always, some of these
are not hard to find out, some of them do only get really annoying once you&#39;re moving into production, or when you get a
deeper knowledge of the tool at hand. Nothing specific to CouchDB here, but some of the issues listed below stem from
actively using it. Take them with a grain of salt. While they may seem annoying at first, they&#39;re things you can live
with. Believe me, you can.</p>

<p><strong>Views are updated on read access</strong></p>

<p>You can dump in as many documents as you want, and you can create as many map/reduce views as you want. The truth is,
they&#39;ll only come all together to slow down your application when you&#39;re querying the view. Assume you have a good stash
of documents in your database, and you decide you need a new view on your data. Throw in the JavaScript functions and go
ahead and query the view. Calling it a slow-down may be a stretch at times though, it really depends on how often your
data is updated.</p>

<p>CouchDB will notice that the B-tree for the view doesn&#39;t exist yet, so it goes ahead and builds it on the first read.
Depending on how many documents you have in your database, that can take a while, putting a good work load on your
database.</p>

<p>On every subsequent read, CouchDB will check if documents have changed since the last write, and throw the changed
documents at the map and reduce function. So if you only query some views from time to time, but have lots of changes in
between, expect some delays on the next read. A way around this would of course be to keep your views warm by reading
them regularly, e.g. through a cron job.</p>

<p>When you add new views, be sure to <a href="http://wiki.apache.org/couchdb/Regenerating_views_on_update">pre-warm</a> them before
you first access them in your application. One way would be to add the views at a time where you database isn&#39;t accessed
as much. It doesn&#39;t block all access to the documents, but it sure has a certain impact on your database&#39;s performance,
and of course the first requests that may time out because CouchDB is building the requested views in the background.</p>

<p>When it comes to just updating a view, and it might take too long, you can set the parameter <code>stale=ok</code>. That way, even
if the view data needs to be updated, CouchDB won&#39;t update it and just return the last known state of the view&#39;s B-tree.</p>

<p>That&#39;s all fun and giggles, but when on earth are you supposed to actually update your view? Always reading stale data
is not great? I&#39;ve gotten some odd suggestions when I complained about this elsewhere, but in the end I just want to
tell the database that I&#39;m okay with stale data, but that it should update the view in the background.</p>

<p><strong>No automatic compaction</strong></p>

<p>As your database grows and data gets updated, CouchDB leaves old and stale data untouched, appending new data (inserted
and updated documents are considered new data) to the end of its database files, a fact that&#39;s also true for view files.
That has the neat advantage that you can still access old revisions of your documents, but it will also leave your
database files growing constantly. Now, depending on the number of documents and updates on them, that might not be a
big deal, but it&#39;s a good idea to start regular compaction earlier than later.</p>

<p><a href="http://downloads.basho.com/papers/bitcask-intro.pdf">Riak&#39;s Bitcask file backend</a> has a neat way of automatically
compacting its files. It appends data in a similar manner as CouchDB, but can determine if a node in the cluster can run
compaction on its data, and do so automatically, without much need for human intervention. It&#39;d be nice to have
something similar as part of CouchDB without having to run cron jobs to do that.</p>

<p>The append-only mechanism makes CouchDB bullet-proof, no doubt, you&#39;ll always have consistent data files on your hard
disk, backups are as simple as copying the files elsewhere, or take an EBS volume snapshot at any time. But that level
of data consistency comes with a price, and that&#39;s an ever-growing data file.</p>

<p><strong>No partial updates</strong></p>

<p>Whenever you update a document in CouchDB, you update it as a whole, there&#39;s nothing in between. That kind of makes
sense with the way CouchDB works, but as a user it annoys me from time to time. It seems so pointless fetching and
sending a whole document when I&#39;m just updating one attribute. There&#39;s a <a href="http://tools.ietf.org/html/rfc5789">neat RFC for the PATCH command in
HTTP</a> making the rounds, I&#39;d love to see that end up in CouchDB at some point. No
idea how likely that is, the makers of CouchDB have a weird aversion to using diffs to update data.</p>

<p>Note that I&#39;m not talking about the MongoDB way of setting attributes atomically. I don&#39;t need that, because it simply
doesn&#39;t scale well, especially not with the CouchDB storage model, and you&#39;re not updating data in-place like MongoDB.
It&#39;s more about just being able to send a diff or a minor update than a whole document.</p>

<p>You can somewhat fake this using <a href="http://wiki.apache.org/couchdb/Document_Update_Handlers">update handlers</a> (look at the
view called &quot;in-place&quot;) from CouchDB 0.10 on. It&#39;s pretty neat, but it&#39;s just not the same.</p>

<p><strong>No built-in way to scale up</strong></p>

<p>CouchDB&#39;s replication is unrivaled, no doubt. Being able to replicate any database with any other database at any point
in time makes CouchDB unique, some say it&#39;s the killer feature, and I concur. There&#39;s lot of argueing whether or not
that defines CouchDB as being distributed. In the most traditional sense, at least to me, it sure does, but I&#39;m not here
to nitpick about that. It&#39;s easy to scale out by adding more nodes and setting them up to constantly replicate with
each other, make anyone a master or slave as you like. But there&#39;s no way to distribute write and read access across a
cluster of nodes.</p>

<p><a href="http://tilgovi.github.com/couchdb-lounge/">CouchDB-lounge</a> has been the traditional way to approaching that, but I
never really liked it, because it added more components to the infrastructure. Something like that should really be
built in.  The good news is that <a href="http://cloudant">Cloudant</a> is planning on open-sourcing their clustering solution Open
Cloudant, which will then hopefully become part of CouchDB. A quorum based system for CouchDB would be neat, and it
doesn&#39;t seem too far away.</p>

<p><strong>Pagination is awkward</strong></p>

<p>CouchDB&#39;s B-tree is a leaky abstraction, that&#39;s the conclusion I came to at some point. It has a pretty big impact on
your application&#39;s code, and that&#39;s not necessarily a bad thing. Suddenly you deal with things like conflicts, or simply
updating views on reads. But no other part of your web application will make that as obvious as pagination, a pretty
common and natural part of a web application.</p>

<p>The path of least resistence to get pagination is to use the <a href="http://books.couchdb.org/relax/reference/recipes#Pagination">skip and limit
parameters</a>, but it&#39;s not recommended, as you&#39;ll still be
walking the whole B-tree to determine the number of documents that must be skipped before it can collect the ones you&#39;re
interested in.</p>

<p>The recommended way to do pagination is a bit awkward if you ask me. There&#39;s a good explanation in the <a href="http://roidi.us/d6f6">CouchDB
book</a>, so I&#39;ll spare you repeating it here. But be sure to read it, because understanding that
takes you half way to understanding the B-tree. It may be awkward, and very different from what you&#39;re used to, but
that&#39;s how the B-tree works. It&#39;s not always unicorns and rainbows, sometimes it kinda gets in your way. Trade-offs,
meh.</p>

<p>The simpler alternative would of course be to just use endless pagination, where you let the users just click a more
button instead of clicking through the pages, because you know the last document displayed in your list, and the key
that was used to fetch it. You simply use that key and the last document&#39;s id to step directly into the B-tree where you
left off. You need to remember to fetch one additional document, as CouchDB will return the last document too, or you
can just skip one document, which is acceptable, as skipping just one leaf in a tree is an operation of predictable
performance.</p>

<p><strong>Range queries are awkward</strong></p>

<p>To do a range, you have to specify a start and an end key. That&#39;s the simple part. It starts getting awkward when your
keys get slightly more complex, e.g. when your map function emits arrays. Assume you want to fetch all elements where
the first part of the array matches a particular key, and the second part doesn&#39;t matter, e.g. when you emitted a
timestamp as the second part to keep a natural (in terms of last update for example) order.</p>

<p>Assume your keys look like this: <code>[&#39;123&#39;, &#39;2010/07/21&#39;]</code>, that&#39;s the key format
<a href="http://github.com/peritor/simply_stored">SimplyStored</a> uses to manage associations between documents. To get the range that
only matches the first part of the key, your startkey has to look like this: <code>[&#39;123&#39;]</code>. This will match all documents
having the above key. If you don&#39;t specify an endkey, CouchDB will simply return all documents following that key, so
you need to specify an endkey. The recommended way to do that is to use the following format: <code>[&#39;123&#39;, {}]</code>. That way
you&#39;ll get all documents matching the first part of the key, because <code>{}</code> is considered to be greater than any string
you may have emitted. See the CouchDB wiki on more details on this technique called <a href="http://wiki.apache.org/couchdb/View_collation">view
collation</a>.</p>

<p>Obviously it&#39;s not impossible to do range queries in CouchDB, but it&#39;s slightly awkward. It all goes
<a href="https://issues.apache.org/jira/browse/COUCHDB-834">downhill</a> as soon as you want to fetch only a particular subrange of
the original one, using startkey<em>docid or endkey</em>docid, say for pagination. With the above ranges, they simply don&#39;t
work. Both need a startkey and endkey that is an exact match. The whole point of the above range query is not to care
about the exact start and end key, isn&#39;t it?</p>

<p><strong>No CommonJS available in MapReduce functions</strong></p>

<p>With CouchDB 0.11, CommonJS and all its awesomeness became <a href="http://wiki.apache.org/couchdb/CommonJS_Modules">available in view
functions</a>. I was pretty excited about it, and I still am. However, map
and reduce functions were left out in the cold. Every time I have to write the same piece of JavaScript in a map or reduce
function that I&#39;ve used elsewhere already, I get bitter about that. Sometimes it&#39;s just very basic stuff that I could
easily solve by throwing an existing library at it, but instead I&#39;m cluttering my view code with it over and over again.
And yes, there&#39;s the <code>!code</code> placeholder, but it&#39;s not about throwing an undebuggable mess of code into my view
function, it&#39;s about not repeating myself. <code>!code</code> doesn&#39;t really solve that problem good enough for me.</p>

<p>Word is that it&#39;s got something to do with determining whether files have updated or not, but hey CouchDB, why don&#39;t you
let me worry about that and let me tell you when I think a file I&#39;ve included through CommonJS has been updated? I would
very much appreciate that.</p>

<p><strong>No link-walking between documents</strong></p>

<p>With CouchDB 0.11, map functions got a way to <a href="http://blog.couch.io/post/446015664/whats-new-in-apache-couchdb-0-11-part-two-views">emit other
documents</a> using <code>{_id:
doc.other_id}</code>, but that still doesn&#39;t allow full access to e.g. attributes of said documents. Sometimes that&#39;d just be
handy to have. Sure, you could use embedded documents, but in that case it&#39;d just be a dumb workaround, where I could
just have a way to fetch a document by its identifier and throw some of its attributes at the map function.</p>

<p>Say what you will though, just being able to emit other documents is still pretty cool. Makes querying and fetching
associated documents a bit easier.</p>

<p><strong>All reads go to disk</strong></p>

<p>CouchDB doesn&#39;t cache anything. It does delay commits if you want it to, so that it doesn&#39;t hit the disk on every
document update, but it sure as heck doesn&#39;t cache anything in memory. This is both curse and blessing. It keeps the
memory footprint of CouchDB incredibly small, no doubt. Considering they&#39;re targeting mobile devices it makes a lot of
sense, plus, accessing flash-based storage is a lot cheaper than spinning disks.</p>

<p>But, on the other hand, when I have the memory available, why not use it? I know caching is a hard problem to solve.
CouchDB is also made for high concurrency, no doubt, but my disks aren&#39;t necessarily. Sure, I could buy faster disks,
but if you really think about it, memory is the new disk, plus, tell Amazon to offer faster network storage for EC2,
please do, maybe that&#39;d already help. CouchDB somewhat relies on the file system cache doing its magic to speed up
things, but I really don&#39;t want to rely on magic. You could put an HTTP-level reverse proxy like Varnish in front of
CouchDB though, that&#39;d be a feasable option, but that adds another layer to your infrastructure.</p>

<p>In all seriousness, I&#39;d love to see some caching introduced in CouchDB. I won&#39;t say it&#39;s an easy feature to implement,
because it sure isn&#39;t, but it doesn&#39;t need to be something fancy, I just would like to see CouchDB use some of my memory
for data that&#39;s read more often than it&#39;s written. But until then, Varnish to the rescue!</p>

<p><strong>Error messages are not helping</strong></p>

<p>I&#39;m just gonna post the following snippet from my CouchDB log file, and leave you to it. You tell me how useful it is.
Suffice it to say, I just wish CouchDB would not dump all that Erlang trace into my log, but maybe a useful error
message for a change. It works in some cases, but a lot of times, when the problem usually is as simple as a permissions
problem, you&#39;re left scratching your head.</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">{&lt;0.84.0&gt;,supervisor_report,
 [{supervisor,{local,couch_secondary_services}},
  {errorContext,start_error},
  {reason,
      {&#39;EXIT&#39;,
          {undef,
              [{couch_auth_cache,start_link,[]},
               {supervisor,do_start_child,2},
               {supervisor,start_children,3},
               {supervisor,init_children,2},
               {gen_server,init_it,6},
               {proc_lib,init_p_do_apply,3}]}}},
  {offender,
      [{pid,undefined},
       {name,auth_cache},
       {mfa,{couch_auth_cache,start_link,[]}},
       {restart_type,permanent},
       {shutdown,brutal_kill},
       {child_type,worker}]}]}}
</code></pre></div>
<p><strong>The End</strong></p>

<p>There you go, some annoying things about CouchDB. They&#39;re annoying, but I still like CouchDB a lot. It&#39;s stuff I can
live it, it&#39;s stuff I can work around, it&#39;s stuff that doesn&#39;t have as big an effect in production as it may seem. The
bottom line is, as always, evaluate your tools. The above list is not to be taken as a list of arguments purely against
using CouchDB. Consider them a list of things you need to be aware of, that may or may not be acceptable compared to
what you gain.</p>

<p>In the end, and any way you look at it, CouchDB still kicks butt.</p>

      </div>
      <div class="item_meta">
        <span class="item_tags">
          Tags: 
          
        </span>
      </div>
    </div>
  </article>

</div>

<div class="pagination">
  
    
      <a href="/page29" class="previous">Newer Posts</a>
    
  
  
    <a href="/page31" class="next">Older Posts</a>
  
</div>

       
        <div id="footer">
          <div id="footer_text">
            <a href="/archives.html">Archives</a>, <a href="http://www.paperplanes.de/rss.xml" title="Full-text RSS feed">RSS Feed</a>, &copy; 2007-2014 Mathias Meyer <a href="/imprint.html">Imprint</a>
          </div>
        </div>
      </div>
    </div>

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-46305173-1', 'paperplanes.de');
      ga('send', 'pageview');

    </script>
  </body>
  <script src="//my.hellobar.com/7db1d1ae6111ae95568efbbf8e6a1ee953ad854f.js" type="text/javascript" charset="utf-8" async="async"></script>
</html>
