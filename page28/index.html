<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
  <head>
    <title>paperplanes. mathias meyer.</title>
    <meta name="robots" content="index,follow"/>
    <meta name="mssmarttagspreventparsing" content="true"/>
    <link rel="shortcut icon" href="/images/favicon.gif" type="image/gif" />
    <link rel="icon" href="/images/favicon.gif" type="image/gif" />
    <meta http-equiv="content-type" content="text/html; charset=utf-8"/>
  	<meta name="author" content="Mathias Meyer"/>
    <meta name="dc.title" content="paperplanes. mathias meyer."/>
  	<link rel="start" href="http://www.paperplanes.de" title="paperplanes"/>
    
    <link href="http://www.paperplanes.de/rss.xml" rel="alternate" title="Primary Feed" type="application/rss+xml" />
    <link href="/stylesheets/screen.css" media="screen" rel="Stylesheet" title="paperplanes" type="text/css" />
    <link rel="stylesheet" type="text/css" href="/stylesheets/mobile.css" media="handheld, only screen and (max-device-width: 960px)" />
  </head>
  
  <body id="www-paperplanes-de">
    <div id="head">
      <div id="header-content">
        <a href="/">
          <img src="/images/paperplane.png" id="paperplane">
        </a>
        <div id="about">
          <h1 class="default">Hi, I'm Mathias Meyer, nice to meet you!</h1>
          <h1 class="mobile">Hi, I'm <a href="https://twitter.com/roidrage">Mathias Meyer</a>, I'm the CEO at <a href="https://travis-ci.com">Travis CI</a></h1>
          <p style="color: white" class="about-sub-title default">
            I'm the CEO at <a href="http://travis-ci.com">Travis CI</a>. I like coffee, <a href="https://twitter.com/roidrage">Twitter</a> and <a href="mailto:meyer@paperplanes.de">email</a>.
          </p>
        </div>
      </div>
    </div>

    <div id="box">
      <div id="content">
        <div id="articles">

  <article>
    <div class="item">
      <div class="item_details">
        <h3><a href="/2011/4/25/eventmachine-how-does-it-work.html">EventMachine, How Does It Work?</a></h3>
        <h4><a href="/2011/4/25/eventmachine-how-does-it-work.html" title="EventMachine, How Does It Work?">25 April 2011</a> by <a href="http://twitter.com/roidrage">Mathias Meyer</a></h4>
      </div>
      <div class="item_content">
        <p>At this year&#39;s <a href="http://scottishrubyconference.com/">Scottish Ruby Conference</a>, I gave a talk about
<a href="http://rubyeventmachine.com">EventMachine</a>, <a href="http://eventmachine-scotrubyconf.heroku.com/">slides</a> are available.
Amidst the hype around Node.js it&#39;s too easy to forget that Ruby has had evented I/O libraries for years.
<a href="http://rubyeventmachine.com">EventMachine</a>, <a href="http://tinyclouds.org/libebb/">libebb</a>, <a href="http://rev.rubyforge.org/">rev</a>,
<a href="http://coolio.github.com/">cool.io</a>, to name a few. As a general introduction I recommend reading <a href="http://www.kegel.com/c10k.html">Dan Kegel&#39;s article
on the C10K problem</a>, the problem of handling 10000 server connections on a single
machine. It introduces all the evented approaches that have been implemented in the different operating systems over the
last some 15 years.</p>

<p>In preparation for the talk I got curious about EventMachine&#39;s innards. So I thought it&#39;d be nice to share my findings
with you. Node.js kids, pay attention, this concerns you as well. It may be JavaScript, but in the end Node.js works in
a similar fashion, though it builds on <a href="http://software.schmorp.de/pkg/libev.html">libev</a>, which does most of the
plumbing for the different operating system implementations of non-blocking I/O.</p>

<p>Most of the magic happens inside the C++ part of EventMachine, so now&#39;s as good a time as any to dig into it and find
out how it works. There&#39;ll be code in here, not assembler, but I&#39;ll be throwing constants, standard library functions
and TCP networking bits (from C, not from Ruby) at you. There&#39;s no magic however, and when in doubt, consult the man
pages. You do know about man pages, right? They&#39;re awesome.</p>

<h3>while(true): The Event Loop</h3>

<p>EventMachine is based on the idea of an event loop, which is basically nothing more than an endless loop. The standard
snippet of code you&#39;re wrapping all your evented code is this:</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">EM.run do
  # go forth and handle events
end
</code></pre></div>
<p>You can look at the details of what the method does in <a href="https://github.com/eventmachine/eventmachine/blob/bd77b0503557de565d3cc9b629ab260b4055bc9d/lib/eventmachine.rb#L185-240">its full
glory</a>.
Other than initializing some things, it dives down into the C++ layer immediately, and it&#39;s where most of the magic
happens from now on.</p>

<p>Three C/C++ extension files are of importance,
<a href="https://github.com/eventmachine/eventmachine/blob/6f7885166746e4dca124780432c8315cd57ca89d/ext/rubymain.cpp"><code>ext/rubymain.cpp</code></a>
is the bridge between Ruby and the C code layer. It uses Ruby&#39;s C functions, mostly to convert datatypes for the later
below. It then calls into code defined in
<a href="https://github.com/eventmachine/eventmachine/blob/6f7885166746e4dca124780432c8315cd57ca89d/ext/cmain.cpp"><code>ext/cmain.cpp</code></a>,
which in turn bridges the C and the C++ code.</p>

<p>When you call <code>EM.run</code> to start the event loop, it calls down into the C layer to <code>t_run_machine_without_threads</code>, which
is called as <code>run_machine</code>, and which in turn calls
<a href="https://github.com/eventmachine/eventmachine/blob/6f7885166746e4dca124780432c8315cd57ca89d/ext/em.cpp#L427"><code>EventMachine_t::Run()</code></a>,
whose interesting bits are shown below.</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text"> while (true) {
   _UpdateTime();
   if (!_RunTimers())
     break;

  _AddNewDescriptors();
  _ModifyDescriptors();

  if (!_RunOnce())
    break;
  if (bTerminateSignalReceived)
    break;
}
</code></pre></div>
<p>There&#39;s your event loop, doesn&#39;t look that scary now, right? It basically does five things:</p>

<ul>
<li><p>Update the current time (line 2)</p>

<p>Used in the next step to determine whether a timer should be fired</p></li>
<li><p>Run configured timers (line 3)</p>

<p>All the timers specified through either <code>add_timer</code> or <code>add_periodic_timer</code> are run here. When you add a timer,
EventMachine stores it in a map indexed with the time it&#39;s supposed to fire. This makes checking the list for the ones
that should be fired in the current iteration a cheap operation.</p>

<p><a href="https://github.com/eventmachine/eventmachine/blob/6f7885166746e4dca124780432c8315cd57ca89d/ext/em.cpp#L1008"><code>_RunTimers()</code></a>
iterates over the list of timers until it reaches one entry whose key (i.e. the time it&#39;s supposed to fire) is higher
than the current time. Easy and efficient.</p>

<p>On a side note, <code>_RunTimers</code> always returns true, so it&#39;s a bit weird that the return value is checked.</p></li>
<li><p>Add new descriptors (line 6)</p>

<p>Whenever you open a new server connection, EventMachine adds an object representing the connection and the associated
callbacks to this list. All connections and descriptors created in the last iteration are handled, which basically
includes setting additional options if applicable and add them to the list of active connections.</p>

<p>On the operating system level a descriptor represents a file handle or a socket connection.  When you open a file,
create a connection to another machine or create a server to listen for incoming connections, all of them are
represented by descriptors, which are basically integers pointing into a list maintained by the operating system.</p></li>
<li><p>Modify descriptors (line 7)</p>

<p>Modify existing descriptors, if applicable. This only has any effect when you&#39;re using epoll, which we&#39;ll get to
later.</p></li>
<li><p>Run the event (line 9)</p>

<p>Check all open file descriptors for new input. Read whatever&#39;s available, run the associated event callbacks. The
heart of the event loop, worth taking a closer look below.</p></li>
</ul>

<p>The event loop really is just an endless loop after all.</p>

<h3>Open a Socket</h3>

<p>When you call <code>EM.connect</code> to open a connection to a remote server, the connection will be immediately created, but it
may not finish until later. The resulting connection will have a bunch of properties:</p>

<ul>
<li><p>The descriptor is configured to not block on input and output by setting the socket option <code>O_NONBLOCK</code>. This way
reads will immediately return when there&#39;s no data instead of waiting for some to arrive, and writes don&#39;t necessarily
write all the data they&#39;re given. It also means that a call to
<a href="http://www.kernel.org/doc/man-pages/online/pages/man2/connect.2.html"><code>connect()</code></a> to create a new connection returns
before it&#39;s fully created.</p></li>
<li><p>The Nagle algorithm is disabled to prevent the TCP stack from delaying sending packets by setting <code>TCP_NODELAY</code> on the
socket. The operating system wants to buffer output to send fewer packets. Disabling Nagle causes any writes to be
sent immediately. As EventMachine does internal buffering, it&#39;s preferrable for the data to be really sent when it&#39;s
eventually written to a socket.</p></li>
<li><p>Reuse connections in <code>TIME_WAIT</code> state before they&#39;re fully removed from the networking stack. TCP keeps connections
around for a while, even after they&#39;re closed to ensure that all data from the other side really, really made it to
your end. Nice and all, but in environments with a high fluctuation of connnections, in the range of hundreds to
thousands per second, you&#39;ll run out of file descriptors in no time.</p></li>
</ul>

<p>Opening a socket is an immediate event, it happens as soon as you create a new connection. Running any callbacks on it
won&#39;t happen until the next iteration of the event loop. That&#39;s why it&#39;s safe to e.g. fire up a new HTTP request and
then attach callbacks to it. Even if that wouldn&#39;t be the case, EventMachine&#39;s
<a href="https://github.com/eventmachine/eventmachine/blob/master/docs/DEFERRABLES">Deferrables</a> (not to be confused with
<code>EM.defer</code>) ensure that callbacks are fired even after the original event fired, when they&#39;re added at a later time.</p>

<p>What is immediately called, though, is the <code>post_init</code> method on the connection object.</p>

<p>Opening a network connection is just one thing you can do with EventMachine, but as it&#39;s the one thing you&#39;re most
likely to do when you&#39;re using it, let&#39;s leave it at that.</p>

<h3>Don&#39;t call us, we&#39;ll call you</h3>

<p>Working with asynchronous code in EventMachine usually involves callbacks, unless you work with your own connection
class. Libraries like <a href="https://github.com/igrigorik/em-http-request"><code>em-http-request</code></a> rely on deferrables to
communicate with your application. They&#39;re fired when a HTTP request finished or failed. But how does a library keep
track of data that only comes in bit by bit?</p>

<p>The answer is simply buffering. Which brings us to the core of the event loop, checking sockets for input, which is done
from the ominous <code>_RunOnce()</code> method in the code snippet above. EventMachine can utilize three mechanisms to check
descriptors for new input.</p>

<h3>select(*)</h3>

<p>The default is using <a href="http://www.kernel.org/doc/man-pages/online/pages/man2/select.2.html"><code>select()</code></a>, a standard
system call to check a collection of file descriptors for input, by way of Ruby&#39;s implementation <code>rb_thread_select()</code>,
which wraps the call to <code>select()</code> with a bunch of code ensuring thread safety.</p>

<p>Using <code>select()</code> pretty much works everywhere, and is perfectly fine up to a certain number of file descriptors. If
you&#39;re simply serving an asynchronous web application or API using EventMachine, this may be totally acceptable.</p>

<p>Implementing this way of handling I/O is rather straight-forward, if you look at the
<a href="https://github.com/eventmachine/eventmachine/blob/master/ext/em.cpp#L823-957">implementation</a>. Collect all file
descriptors that may be of interest, feed them into select, read and/or write data when possible.</p>

<p>What makes using <code>select()</code> a bit cumbersome is that you always have to assemble a list of all file descriptors for
every call to <code>_RunOnce()</code>, so EventMachine iterates over all registered descriptors with every loop. After select ran,
it loops over all file descriptors again, checking to see if <code>select</code> marked them as ready for reads and/or writes.</p>

<p>When <code>select()</code> marks a descriptor as ready for read or write operations that means the socket will not block when data
is read from or written to it. In the case of reading that usually means the operating system has some data buffered
somewhere, and it&#39;s safe to read that data without having to wait for it to arrive, which in turn would block the call.</p>

<p>Instead of using <code>select()</code>, EventMachine could also use
<a href="http://www.kernel.org/doc/man-pages/online/pages/man2/poll.2.html"><code>poll()</code></a> instead, which just handles a bit nicer in
general, but is not available in the Ruby VM. </p>

<h3>epoll</h3>

<p><a href="https://github.com/eventmachine/eventmachine/blob/master/docs/EPOLL">epoll</a> is Linux&#39; implementation for multiplexing
I/O across a large number of file descriptors.</p>

<p>The basic steps of using epoll are simple:</p>

<ul>
<li><p>Set up an epoll instance using
<a href="http://www.kernel.org/doc/man-pages/online/pages/man2/epoll_create.2.html"><code>epoll_create</code></a>, done initially when the
event loop is created. This creates a virtual file descriptor pointing to a data structure that keeps track of all
real file descriptors associated with it in the next step.</p>

<p>You only need to reference this single file descriptor later, so there&#39;s no need to collect a list of all file
descriptors, as is the case when <code>select()</code> is used.</p></li>
<li><p>Register interest for events on a file descriptor using
<a href="http://www.kernel.org/doc/man-pages/online/pages/man2/epoll_ctl.2.html"><code>epoll_ctl</code></a> on the epoll instance created
above.</p>

<p>This is used in <code>_AddNewDescriptors</code> and <code>_ModifyDescriptors</code> to register and update EventMachine&#39;s file descriptors
with epoll. In fact, both methods only do anything noteworthy when epoll is used. Otherwise they just iterate over a
list of descriptors, pretty much doing nothing with them.</p></li>
<li><p>Wait for input with <a href="http://www.kernel.org/doc/man-pages/online/pages/man2/epoll_wait.2.html"><code>epoll_wait</code></a> for a
specified duration. You can wait forever, return immediately if nothing happened, or wait for a specific amount of
time.</p>

<p>EventMachine seems to have chosen to return immediately if there&#39;s no activity. There&#39;s an alternative implementation
calculating the time to wait based on the likelihood of a specific event (e.g a timer firing) to fire on the next
event loop iteration, but it doesn&#39;t seem to ever be used. Seems to be a relict from the time it could also be used as
a C++ library.</p></li>
</ul>

<p>epoll events are registered for both reads and writes, with <code>epoll_wait</code> returning the number of file descriptors that
are ready for both events.</p>

<p>Using epoll has a big advantage, aside from being much more efficient than select in general for larger sets of file
descriptors. It spares code using it the burden of constantly iterating over a list of file descriptors. Instead you
just register them once, and then only iterate over the ones affected by the last call to <code>epoll_wait</code>.</p>

<p>So epoll requires a bit more work when you add or modify connections, but is a bit nicer on the eyes when it comes to
actually polling them for I/O availability.</p>

<p>Note that epoll support must be explicitly enabled using <code>EM.epoll</code>.</p>

<h3>kqueue</h3>

<p>kqueue is the BSD equivalent of epoll, and is available on e.g. FreeBSD and Mac OS X. It works very similar to epoll. If
you want to know more details, I&#39;d suggest <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.79.3925&amp;rep=rep1&amp;type=pdf">reading the paper on it by Jonathan
Lemon</a>.</p>

<p>You can enable kqueue support using <code>EM.kqueue</code>, which is, just like <code>EM.epoll</code>, a noop on systems that don&#39;t support
it. Hopefully future EM versions will use whatever&#39;s available on a particular system as default.</p>

<h3>Call me already!</h3>

<p>All three mechanisms used have one thing in common: when data is read, <code>receive_data</code> is called immediately, which
brings us back to the question of how a connection objects collects data coming in.</p>

<p>Whenever data is ready to be consumed from a socket, EventMachine calls <code>EventDescriptor::Read()</code>, which reads a bunch
of data from the socket, in turn calling <a href="http://www.kernel.org/doc/man-pages/online/pages/man2/read.2.html"><code>read()</code></a>
on the file descriptor, and then immediately executes the callback associated with the descriptor, which usually ends up
calling <code>receive_data</code> with the data that was just read. Note that the callback here refers to something defined on the
C++ level, not yet a Ruby block you&#39;d normally use in an asynchronous programming model.</p>

<p><code>receive_data</code> is where you will usually either buffer data or run some action immediately. em-http-request feeds the
data coming in directly into an HTTP parser. Whatever you do in here, make it quick, don&#39;t process the data for too
long. A common pattern in libraries built on EventMachine is to use a <code>Deferrable</code> object to keep track of a request&#39;s
state, firing callbacks when it either succeeded or failed.</p>

<p>Which brings me to the golden rule of programming with libraries like EventMachine and Node.js: DON&#39;T BLOCK THE EVENT
LOOP!! Defer whatever work you can to a later run of the loop when it makes sense, or push it to another asynchronous
processing facility, e.g. a message queue like RabbitMQ or Redis&#39; Pub/Sub.</p>

<p>In a similar fashion, whenever you write data to a connection using <code>send_data</code>, it&#39;s first buffered, and not actually
sent until the socket is ready for a non-blocking call to
<a href="http://www.kernel.org/doc/man-pages/online/pages/man2/write.2.html"><code>write()</code></a>. Hence all three implementations check
for both read and write availability of a descriptor.</p>

<h3>Fibers vs. Spaghetti</h3>

<p>Where do Ruby&#39;s Fibers come in here? Callbacks can easily lead to spaghetti code, especially when you have to nest them
to run multiple asynchronous actions in succession.</p>

<p>Fibers can stop execution of a process flow at any time and yield control to some other, controlling entity or another
Fiber. You could, for example, wrap a single HTTP request into a fiber and yield back control when all the callbacks
have been assigned.</p>

<p>In the callbacks you then resume the Fiber again, so that processing flow turns into a synchronous, procedural style
again.</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">  def get(url)
    Fiber.new do
      current_fiber = Fiber.current
      request = EM::HttpRequest.new(url).get
      request.callback { current_fiber.resume(request) }
      request.errback  { current_fiber.resume(request) }
      Fiber.yield
    end
  end
</code></pre></div>
<p><code>Fiber.yield</code> returns whatever object it was handed in <code>Fiber.resume</code>. Wrap this in a method and boom, there&#39;s your
synchronous workflow. Now all you need to do is call <code>get(&#39;http://paperplanes.de&#39;)</code> and assign something with the return
value. Many props to <a href="http://rubysource.com/understanding-concurrent-programming-with-ruby-goliath/">Xavier Shay for digging into the Goliath
source</a> to find out how that stuff works.
Helped me a lot to understand how that stuff works.  If you never had a proper use case for Fibers in real life, you do
now.</p>

<p><a href="https://github.com/igrigorik/em-synchrony"><code>em-synchrony</code></a> is a library doing just that for a lot of existing EventMachine libraries, and
<a href="http://goliath.io">Goliath</a> is an evented web server, wrapping a Rack-style API using Fibers.</p>

<h3>Things you should be reading</h3>

<p>Here&#39;s a bunch of free reading tips for ya. These books are pretty old, but have gone through some revisions and
updates, and they&#39;re still the classics when it comes to lower level Unix (network) programming and understanding
TCP/IP, which I consider very important. TCP/IP Illustrated is one of the best books I&#39;ve read so far, and I consider it
essential knowledge to be aware of what happens under the networking hood.</p>

<ul>
<li><a href="http://www.amazon.com/gp/product/0201633469/ref=as_li_tf_tl?ie=UTF8&amp;tag=javaddicts-20&amp;linkCode=as2&amp;camp=217145&amp;creative=399353&amp;creativeASIN=0201633469">TCP/IP Illustrated Vol.  1</a></li>
<li><a href="http://www.amazon.com/gp/product/0131411551/ref=as_li_tf_tl?ie=UTF8&amp;tag=javaddicts-20&amp;linkCode=as2&amp;camp=217145&amp;creative=399353&amp;creativeASIN=0131411551">Unix Network Programming</a></li>
<li><a href="http://www.amazon.com/gp/product/0321525949/ref=as_li_tf_tl?ie=UTF8&amp;tag=javaddicts-20&amp;linkCode=as2&amp;camp=217145&amp;creative=399353&amp;creativeASIN=0321525949">Advanced Programming in the Unix
Environment</a></li>
</ul>

<p>Also, read the fine man pages. There&#39;s a whole bunch of good documentation installed on every Unix-style system, and I
linked to a couple of them relevant to this post already. Read it.</p>

<h3>yield</h3>

<p>This concludes today&#39;s whirlwind tour through some of EventMachine&#39;s internals. There&#39;s actually not too much magic
happening under the covers, it&#39;s just wrapped into a bit too much code layering for my taste. But you be the judge.</p>

<p>Play with EventMachine and/or Node.js if you haven&#39;t already, try to wrap your head around the asynchronous programming
model. But for the love of scaling, don&#39;t look at evented and asynchronous I/O as the sole means of scaling, because
it&#39;s not.</p>

      </div>
      <div class="item_meta">
        <span class="item_tags">
          Tags: 
          
        </span>
      </div>
    </div>
  </article>

  <article>
    <div class="item">
      <div class="item_details">
        <h3><a href="/2011/1/10/mongodb_and_data_durability.html">MongoDB, Data Durability and Improvements coming in 1.8</a></h3>
        <h4><a href="/2011/1/10/mongodb_and_data_durability.html" title="MongoDB, Data Durability and Improvements coming in 1.8">10 January 2011</a> by <a href="http://twitter.com/roidrage">Mathias Meyer</a></h4>
      </div>
      <div class="item_content">
        <p>Last weekend I <a href="http://twitter.com/#!/roidrage/status/23779182852968449">tweeted two links</a> to
<a href="http://twitter.com/#!/mikemaccana/status/23397112360206337">two</a>
<a href="http://twitter.com/#!/mikemaccana/status/23703196522254337">tweets</a> by a poor guy who apparently got his MongoDB
database into an unrecoverable state during shutdown whilst upgrading to a newer version. That tweet quickly made the
rounds, and the next morning I saw myself staring at replies stating that it was all his fault, because he 1.) used kill
-9 to shut it down because apparently the process hung (my guess is it was in the middle of flushing all data to disk)
and 2.) didn&#39;t have a slave, just one database instance.</p>

<p>Others went as far as <a href="http://twitter.com/#!/MacYET/status/24029811332612096">indirectly calling him an
idiot</a>. Oh interwebs, you make me sad. If you check out the
<a href="http://groups.google.com/group/mongodb-user/t/d0111a47984cb688">thread on the mailing list</a>, you&#39;ll notice a similar
pattern in reasoning. The folks over at <a href="http://learnmongo.com">http://learnmongo.com</a> seem to want to be <a href="http://twitter.com/#!/LearnMongo/status/23917393143140352">the wittiest of them
all</a>, recommending to always have a recent backup, a slave or
replica set and to never <code>kill -9</code> your database.</p>

<p>While you can argue that the guy should&#39;ve known better, there&#39;s something very much at odds here, and it seems to
become a terrifying meme with fans of MongoDB, the idea that you need to do all of these things to get the insurance of
your data being durable. Don&#39;t have a replica? Your fault. <code>kill -9</code> on a database, any database? You mad? Should&#39;ve
read the documentation first, dude. This whole issue goes a bit deeper than just reading documentation, it&#39;s the
fundamental design decision of how MongoDB treats your data, and it&#39;s been my biggest gripe from the get go. I can&#39;t
help but be horrified by these comments.</p>

<p>I&#39;ve heard the same reasoning over and over again, and also that it just hasn&#39;t happened so far, noone&#39;s really lost any
considerable data. The problem is, most people never talk about it publicly, because it&#39;s embarrassing, best proof is
the poor guy above. This issue is not even related to MongoDB, it&#39;s a general problem.</p>

<h3>Memory-Mapped Persistence</h3>

<p>But let me start at the beginning, MongoDB&#39;s persistence cycle, and then get to what&#39;s being done to improve its
reliability and your data&#39;s durability. At the very heart, MongoDB uses <a href="http://en.wikipedia.org/wiki/Memory-mapped_file">memory-mapped
files</a> to store data. A memory-mapped file is a data structure that has
the same representation on disk as it has when loaded into memory. When you access a document in MongoDB, loading it
from disk is transparent to MongoDB itself, it can just go ahead and write to the address in memory, as every database
in MongoDB is mapped to a dynamically allocated set of files on disk. Note that memory-mapped files are something you
won&#39;t find in a lot of other databases, if any at all. Most do their own house-keeping and use custom data structures
for that purpose.</p>

<p>The memory mapping library (in MongoDB&#39;s case the POSIX functions, and whatever Windows offers in that area) will take
care of handling the flush back to disk every 60 seconds (configurable). Everything in between happens solely in memory.
Database crash one second before the flush strikes again? You just lost most of the data that was written in the last 59
seconds. Just to be clear, the flushing cycle is configurable, and you should consider choosing a better value depending
on what kind of data you&#39;re storing.</p>

<p>MongoDB&#39;s much praised insert speed? This is where it comes from. When you write stuff directly to local memory, they
better be fast. The persistence cycle is simple: accept writes for 60 seconds, then flush the whole thing to disk. Wait
for another 60 seconds, then flush again, and so on. Of course MongoDB also flushes the data when you shut it down. But,
and here&#39;s the kicker, of course that flush will fail when you kill it without mercy, using the KILL signal, just like
the poor guy above did apparently. When you kill something that writes a big set binary data to disk, all bets are off.
One bit landing on the wrong foot and the database can get corrupted.</p>

<h3>Database Crashes are Unavoidable</h3>

<p>This scenario can and does happen in e.g. MySQL too, it even happens with CouchDB, but the difference is, that in MySQL
you usually only have a slightly damaged region, which can be fixed by deleting and re-inserting it. In CouchDB, all
that happens is that your last writes may be broken, but CouchDB simply walks all the way back to the last successful
write and runs happily ever after.</p>

<p>My point here is simple: even when killed using the KILL signal, a database should not be unrecoverable. It simply
shouldn&#39;t be allowed to happen. You can blame the guy all you want for using <code>kill -9</code>, but consider the fact that it&#39;s
the process equivalent of a server or even just the database process crashing hard. Which happens, believe it or not.</p>

<p>Yes, you can and probably will have a replica eventually, but it shouldn&#39;t be the sole precondition to get a durable
database. And this is what horrifies me, people seem to accept that this is simply one of MongoDB&#39;s trade-offs, and that
it should just be considered normal. They shouldn&#39;t, it needs more guys like the one causing all the stir bringing up
these isses, even though it&#39;s partly his fault, to show the world what can happen when worse comes to worst.</p>

<p>People need to ask more questions, and not just accept answers like: don&#39;t use kill -9, or always have a replica around.
Servers crash, and your database needs to be able to deal with it.</p>

<h3>Durability Improvements in MongoDB 1.7/1.8</h3>

<p>Now, the MongoDB folks aren&#39;t completely deaf, and I&#39;m happy to report they&#39;ve been working on improvements in the area
of data durability for a while, and you can play with the new durability option <a href="http://www.mongodb.org/downloads">in the latest builds of the 1.7
branch</a>, and just a couple of hours ago, there <a href="https://github.com/mongodb/mongo/commit/6485f8b9f3092bfb2d520adbd54a7809a047cc22">was activity in improving the repair
tools</a> to better deal with corrupted
databases. I welcome these changes, very much so. MongoDB has great traction, a pretty good feature set, and the speed
seems to blow peoples&#39; minds. Data durability has not been one of its strengths though, so I&#39;m glad there&#39;s been a lot
of activity in that area.</p>

<p>If you start the MongoDB server with the new <code>--dur</code> option, and it will start keeping a journal. When your database
crashed, the journal is simply replayed to restore all changes since the last successful flush. This is not a
particularly special idea, because it&#39;s how your favorite relation database has been working for ages, and not unsimilar
to the storage model of other databases in the NoSQL space. It&#39;s a good trade-off between keeping good write speed and
getting a much more durable dataset.</p>

<p>When you kill your database harshly in between flushes with a good pile of writes in between, you don&#39;t lose a lot of
data anymore, maybe a second&#39;s worth (just as you do with MySQL when you use InnoDB&#39;s delayed flushing), if any at all,
but not much more than that. Note that these are observation based on a build that&#39;s now already more than a month old.
Situation may have improved since then. Operations are put into a buffer in memory, from where they&#39;re both logged to
disk into the journal, and then applied to the dataset. When writing the data to memory, it has already been written to
the journal. Journals are rotated once they reach a certain size and it&#39;s ensured that all their data has been applied
to the dataset.</p>

<p>A recovery process applies all uncommitted changes from the log when the database crashes. This way it&#39;s ensured that
you only lose a minimum set of data, if none at all, when your database server crashes hard. In theory the journal could
be used to restore a corrupted in a scenario as outlined above, so it&#39;s pretty neat in my opinion. Either way, the risk
of losing data is now pretty low. In case your curious for code, the magic happens in <a href="https://github.com/mongodb/mongo/blob/master/db/dur.cpp#L419-461">this
method</a>.</p>

<p>I for one am glad to see improvements in this area of MongoDB, and I&#39;m secretly hoping that durable will become the
default mode, though I don&#39;t see it happening for marketing reasons anytime soon. Also, be aware that durability brings
more overhead. In some initial tests however, the speed difference between non-durable and durable MongoDB was almost
not worth mentioning, though I wouldn&#39;t call them representative, but in general there&#39;s no excuse to not use it really.</p>

<p>It&#39;s not yet production ready, but nothing should keep you from playing with it to get an idea of what it does.</p>

<h3>Bottom Line</h3>

<p>It&#39;s okay to accept trade-offs with whatever database you choose to your own liking. However, in my opinion, the
potential of losing all your data when you use <code>kill -9</code> to stop it should not be one of them, nor should accepting
that you always need a slave to achieve any level of durability. The problem is less with the fact that it&#39;s MongoDB&#39;s
current way of doing persistence, it&#39;s with people implying that it&#39;s a seemingly good choice. I don&#39;t accept it as
such. If you can live with that, which hopefully you don&#39;t have to for much longer anyway, that&#39;s fine with me, it&#39;s not
my data anyway. Or maybe I&#39;m just too paranoid.</p>

      </div>
      <div class="item_meta">
        <span class="item_tags">
          Tags: 
          
        </span>
      </div>
    </div>
  </article>

  <article>
    <div class="item">
      <div class="item_details">
        <h3><a href="/2011/1/5/the_virtues_of_monitoring.html">The Virtues of Monitoring</a></h3>
        <h4><a href="/2011/1/5/the_virtues_of_monitoring.html" title="The Virtues of Monitoring">05 January 2011</a> by <a href="http://twitter.com/roidrage">Mathias Meyer</a></h4>
      </div>
      <div class="item_content">
        <p>Over the last year I haven&#39;t only grown <a href="http://holgarific.net/?p=642">very fond of coffee</a>, but also of infrastructure.
Working on <a href="http://scalarium.com">Scalarium</a> has been a fun ride so far, for all kinds of reasons, one of them is
dealing so much with infrastructure. Being an infrastructure platform provider, what can you do, right?</p>

<p>As being responsible for deployment, performance tuning, monitoring, infrastructure has always been a part of many of my
job I thought it&#39;d be about time to sprinkle some of my thoughts and daily ops thoughts on a couple of articles. The
simple reason being that no matter how much you try, no matter how far away from dealing with servers you go (think
Heroku), there will always be infrastructure, and it will always affect you and your application in some way.</p>

<p>On today&#39;s menu: monitoring. People have all kinds of different meanings for monitoring, and they&#39;re all right, because
there is no one way to monitor your applications and infrastructure. I just did a recount, and there are no less than
six levels of detail you can and probably should get. Note that these are my definitions, they don&#39;t necessarily have
to be officially named, they&#39;re solely based on my experiences. Let&#39;s start from the top, the outside view of your
application.</p>

<h3>Availability Level</h3>

<p>Availability is a simple measure to the user, either your site is available or it&#39;s not. There is nothing in between.
When it&#39;s slow, it&#39;s not available. It&#39;s a beautifully binary measure really. From your point of view, any component or
layer in your infrastructure could be the problem. The art is to quickly find out which one it is.</p>

<p>So how do you notice when your site is not available? Waiting for your users to tell you is an option, but generally a
pretty embarrassing one. Instead you generally start polling some part of your site that&#39;s representative of it as a
whole. When that particular site is not available, your whole application may as well not be.</p>

<p>What that page should do is get a quick measure of the most important components of your site, check if they&#39;re
available (maybe even with a timeout involved so you get an idea if a specific component is broken) and return the
result. An external process can then monitor that page and notify you when it doesn&#39;t return the expected result. Make
sure the site does a bit more than just return &quot;OK&quot;. If it doesn&#39;t hit any of the major components in your stack,
there&#39;s a chance you&#39;re not going to notice that e.g. your database is becoming unavailable.</p>

<p>You should run this process from a different host, but what do you do if that host is not available? Even as an
infrastructure provider I like outsourcing parts of my own infrastructure. Here&#39;s where <a href="http://pingdom.com">Pingdom</a>
comes into play. They can monitor a specific URL, TCP ports and whatnot from some two dozen locations across the planet
and they randomly go through all of them, notifying you when your site is unavailable or the result doesn&#39;t
match the expectations.</p>

<p><img src="https://img.skitch.com/20110105-eb51gw3pqrqbfxd7cc5mj6i9eh.jpg" alt="Pingdom"></p>

<h3>Business Level</h3>

<p>These aren&#39;t necessarily metrics related to your application&#39;s or infrastructure&#39;s availability, they&#39;re more along the
lines of what your users are doing right now, or have done over the last month. Think number of new users per day,
number of sales in the last hour, or, in our case, number of EC2 instances running at any minute.  Stuff like Google
Analytics or click paths (using tools like <a href="http://projects.nuttnet.net/hummingbird/">Hummingbird</a>, for example) in
general also fall into this category.</p>

<p>These kind of metrics may be more important to your business than to your infrastructure, but they&#39;re important
nonetheless, and they could e.g. be integrated with another metrics collection tool, some of which we&#39;ll get to in a
minute. Depending on what kind of data you&#39;re gathering they&#39;re also useful to analyze spikes in your application&#39;s
performance.</p>

<p>This kind of data can be hard to track in a generic way. Usually it&#39;s up to your application to gather them and turn
them into a format that&#39;s acceptable to a different tool to collect them. They&#39;re also usually very specific to your
application and its business model.</p>

<h3>Application Level</h3>

<p>Digging deeper from the outsider&#39;s view, you want to be able to track what&#39;s going on inside of your application right
now. What are the main entry points, what are the database queries involved, where are the hot spots, which queries are
slow, what kinds of errors are being caused by your application, to name a few.</p>

<p>This will give you an overview of the innards of your code, and it&#39;s simply invaluable to have that kind of insight. You
usually don&#39;t need much historical data in this area, just a couple of days worth will usually be enough to analyze
problems in retrospect. It can&#39;t hurt to keep them around though, because growth also shows trends in potential
application code hot spots or database queries getting slower over time.</p>

<p>To get an inside view of your application, services like <a href="http://newrelic.com/">New Relic</a> exist. While their services
aren&#39;t exactly cheap (most monitoring services aren&#39;t, no surprise here), they&#39;re invaluable. You can dig down from the
Rails controller level to find the method calls and database queries that are slowest at a given moment in time (most
likely you&#39;ll be wanting to check the data for the last hours to analyze an incident), digging deeper into other metrics
from there. Here&#39;s an example of what it looks like.</p>

<p><img src="https://img.skitch.com/20110105-n7fhy9ijt7g8rmbq1p6fp2gf5p.jpg" alt="New Relic"></p>

<p>You can also use the Rails log file and tools like
<a href="https://github.com/wvanbergen/request-log-analyzer">Request-log-analyzer</a>. They can help you get started for free, but
don&#39;t expect a similar, fine-grained level of detail like you get with New Relic. However, with Rails 3 it&#39;s become a
lot easier to instrument code that&#39;s interesting to you and gather data on runtimes of specific methods yourself.</p>

<p>Other means are e.g. JMX, one of the neat features you get when using a JVM-based language like JRuby. Your application
can contiuously collect and expose metrics through a defined interface to be inspected or gathered by other means. JMX
can even be used to call into your application from the outside, without having to go through a web interface.</p>

<p>Application level monitoring also includes exception reporting. Services like <a href="http://getexceptional.com">Exceptional</a>
or <a href="http://airbrake.io">Airbrake Bug Tracker</a> are probably the most well known in that area, though in higher price regions New
Relic also includes exception reporting.</p>

<h3>Process Level</h3>

<p>Going deeper (closer to inception than you think) from the application level we reach the processes that serve your
application. Application servers, databases, web servers, background processing, they all need a process to be
available.</p>

<p>But processes crash. It&#39;s a bitter and harsh truth, but they do, for whatever reason, maybe they consumed too many
resources, causing the machine to swap or the process to simply crash because the machine doesn&#39;t have any memory left
to allocate. Think of a memory leaking Rails application server process or the last time you used RMagick.</p>

<p>Someone must ensure that the processes keep running or that they don&#39;t consume more resources than they&#39;re allowed to,
to ensure availability on that level. These tools are called supervisors. Give them a pid file and a process, running or
not, and they&#39;ll make sure that it is. Whether a process is running can depend on multiple metrics, availability over
the network, a file size (think log files) or simply the existence of the process, while allowing you to send some sort
of grace period, so they&#39;ll retry a number of times with a timeout before actually restarting the process or giving up
monitoring it altogether.</p>

<p>A good supervisor will also let you alert someone when the expected conditions move outside or their acceptable
perimeter and a process had to be restarted. A classic in this area is <a href="http://mmonit.com/monit/">Monit</a>, but people
also like <a href="http://god.rubyforge.org/">God</a> and <a href="https://github.com/arya/bluepill">Bluepill</a>. On a lower level you have
tools like <a href="http://smarden.org/runit/">runit</a> or <a href="http://upstart.ubuntu.com/">upstart</a>, but their capabilities are
usually built around a pid file and a process, not allowing to go on a higher level of checking system resources.</p>

<p>While I find the syntax of Monit&#39;s configuration to not be very aesthetically pleasing, it&#39;s proven to be reliable and
has a very small footprint on the system, so it&#39;s our default on our own infrastructure, and we add it to most our
cookbooks for <a href="http://scalarium.com">Scalarium</a>, as it&#39;s installed on all managed instances anyway. It&#39;s a matter of
preference.</p>

<h3>Infrastructure/Server Level</h3>

<p>Another step down from processes we reach the system itself. CPU and memory usage, load average, disk I/O, network
traffic, are all traditional metrics collected on this level. The tools (both commercial and open source) in this area
can&#39;t be counted. In the open source world, the main means to visualize these kinds of metrics is rrdtool. Many tools
use it to graph data and to keep an aggregated data history around, using averages for hours, days or weeks to store the
data efficiently.</p>

<p>This data is very important in several ways. For one, it will show you what your servers are doing right now, or in the
last couple of minutes, which is usually enough to notice a problem. Second, the data collected is very useful to
discover trends, e.g. memory usage increasing over time, swap usage increasing, or a partition running out of disk
space. Any value constantly increasing over time is a good sign that you&#39;ll hit a wall at some point. Noticing trends
will usually give you a good indication that something needs to be changed in your infrastructure or your application.</p>

<p><img src="https://img.skitch.com/20110105-9khif3pghuefyucbkb7ayn62t.jpg" alt="Munin"></p>

<p>There&#39;s a countless number of tools in this area, <a href="http://munin-monitoring.org/">Munin</a> (see screenshot),
<a href="http://www.nagios.org/">Nagios</a>, <a href="http://ganglia.sourceforge.net/">Ganglia</a>, <a href="http://collectd.org/">collectd</a> on the
open source end, and <a href="http://cloudkick.com/">CloudKick</a>, <a href="http://circonus.com/">Circonus</a>, <a href="http://www.serverdensity.com/">Server
Density</a> and <a href="https://scoutapp.com/">Scout</a> on the paid service level, and an abundance
of commercial tools on the very expensive end of server monitoring. I never really bother with the commercial ones, because I
either resort to the open source tools or pay someone to take care of the monitoring and alerting for me on a service
basis. Most of these tools will run some sort of agent on every system, collecting data in a predefined cycle, delivering it to
a master process, or the master processing picking up the data from the agents.</p>

<p>Again, it&#39;s a matter of taste. Most of the open source tools available tend to look pretty ugly on the UI part, but if
the data and the graphs are all that matters to you, they&#39;ll do just fine. We do our own server monitoring using <a href="http://serverdensity.com">Server
Density</a>, but on <a href="http://scalarium.com">Scalarium</a> we resort to using Ganglia as an integrated
default, because it&#39;s much more cost effective on our users, and given the elastic nature of EC2 it&#39;s much easier for us
to add and remove instances as they come and go. In general I&#39;m also a fan of Munin.</p>

<p>Most of them come with some sort of alerting that allows you to define thresholds which trigger the alerts. You&#39;ll never
get the thresholds right the first time you configure them, constantly keep an eye on them to get a picture of what
thresholds are normal, and which are indeed problem areas and require an alert to be triggered.</p>

<p>The beauty about these tools is that you can throw any metric at them you can think of. They can even be used to collect
business level data, utilizing the existing graphing and even alerting capabilities.</p>

<h3>Log Files</h3>

<p>The much dreaded log file won&#39;t go out of style for a long time, that&#39;s for sure. Your web server, your database, your
Rails application, your application server, your mail server, all of them dump more or less useful information into log
files. They&#39;re usually the most immediate and uptodate view of what&#39;s going on in your application, if you chose to
actually log something, Rails appliations traditionally seem to be less of a candidate here, but your background
services sure are, or any other service running on your servers. The log is the first to know when there&#39;s problems
delivering email or your web server is returning an unexpected amount of 500 errors.</p>

<p>The biggest problem however is aggregating the log data, centralized logging if you will. syslog and all the alternative
tools are traditionally sufficient, while on the larger scale end you have custom tools like Cloudera&#39;s
<a href="https://github.com/cloudera/flume">Flume</a> or Facebook&#39;s <a href="https://github.com/facebook/scribe">Scribe</a>. There&#39;s also
a bunch of paid services specializing on logging, most noteworthy are <a href="http://www.splunk.com/">Splunk</a> and
<a href="http://loggly.com">Loggly</a>. Loggly relies on syslog to collect and transmit data from your servers, but they also have
a custom API to transmit data. The data is indexed and can easily be searched, which is usually exactly what you want to
do with logs. Think about the last time you grepped for something in multiple log files, trying to narrow down the data
found to a specific time frame.</p>

<p>There&#39;s a couple of open source tools available too, <a href="http://www.graylog2.org/">Graylog2</a> is a syslog server with a
MongoDB backend and a Java server to act as a syslog endpoint, and a web UI allowing nicer access to the log data. A bit
more kick-ass is <a href="http://code.google.com/p/logstash/">logstash</a> which uses RabbitMQ and ElasticSearch for indexing and
searching log data. Almost like a self-hosted Loggly.</p>

<p>When properly aggregated log files can show trends too, but aggregating them gets much harder the more log data your
infrastructure accumulates. </p>

<h3>ZOMG! So much monitoring, really?</h3>

<p>Infrastructure purists would start by saying that there&#39;s a different between monitoring, metrics gathering and log
files. To me, they&#39;re a similar means to a similar end. It doesn&#39;t exactly matter what you call it, the important thing
is to collect and evaluate the data.</p>

<p>I&#39;m not suggesting you need every single kind of logging, monitoring and metrics gathering mentioned here. There is
however one reason why eventually you&#39;ll want to have most if not all of them. At any incident in your application or
infrastructure, you can correlate all the available data to find the real reason for a downtime, a spike or slow
queries, or problems introduced by recent deployments.</p>

<p>For example, your site&#39;s performance is becoming sluggish in certain areas, users start complaining. Application level
monitoring indicates specific actions taking longer than usual, pointing to a specific query. Server monitoring
for your database master indicates an increased number of I/O waits, usually a sign that too much data is read from or
written to disk. Simplest reason could be an index missing or that your data doesn&#39;t fit into memory anymore and too
much of it is swapped out to disk. You&#39;ll finally be looking at MySQL&#39;s slow query log (or something similar for your
favorite database) to find out what query is causing the trouble, eventually (and hopefully) fixing it.</p>

<p>That&#39;s the power of monitoring, and you just can&#39;t put any price on a good setup that will give you all the data and
information you need to assess incidents or predict trends. And while you can set up a lot of this yourself, it doesn&#39;t
hurt to look into paid options. Managing monitoring yourself means managing more infrastructure. If you can afford to
pay someone else to do it for you, look at some of the mentioned services, which I have no affiliation with, I just
think they&#39;re incredibly useful.</p>

<p>Even being an infrastructure enthusiast myself, I&#39;m not shy of outsourcing where it makes sense. Added features like SMS
alerts, iPhone push notifications should also be taken into account. Remember that it&#39;d be up to you to implement all
this. It&#39;s not without irony that I mention <a href="http://www.pagerduty.com/">PagerDuty</a>. They sit on top of all the other
monitoring solutions you have implemented and just take care of the alerting, with the added benefit of on-call
schedules, alert escalation and more.</p>

      </div>
      <div class="item_meta">
        <span class="item_tags">
          Tags: 
          
        </span>
      </div>
    </div>
  </article>

</div>

<div class="pagination">
  
    
      <a href="/page27" class="previous">Newer Posts</a>
    
  
  
    <a href="/page29" class="next">Older Posts</a>
  
</div>

       
        <div id="footer">
          <div id="footer_text">
            <a href="/archives.html">Archives</a>, <a href="http://www.paperplanes.de/rss.xml" title="Full-text RSS feed">RSS Feed</a>, &copy; 2007-2014 Mathias Meyer <a href="/imprint.html">Imprint</a>
          </div>
        </div>
      </div>
    </div>

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-46305173-1', 'paperplanes.de');
      ga('send', 'pageview');

    </script>
  </body>
  <script src="//my.hellobar.com/7db1d1ae6111ae95568efbbf8e6a1ee953ad854f.js" type="text/javascript" charset="utf-8" async="async"></script>
</html>
