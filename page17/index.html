<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
  <head>
    <title>paperplanes. mathias meyer.</title>
    <meta name="robots" content="index,follow"/>
    <meta name="mssmarttagspreventparsing" content="true"/>
    <link rel="shortcut icon" href="/images/favicon.gif" type="image/gif" />
    <link rel="icon" href="/images/favicon.gif" type="image/gif" />
    <meta http-equiv="content-type" content="text/html; charset=utf-8"/>
  	<meta name="author" content="Mathias Meyer"/>
    <meta name="dc.title" content="paperplanes. mathias meyer."/>
  	<link rel="start" href="http://www.paperplanes.de" title="paperplanes"/>
    
    <link href="http://www.paperplanes.de/rss.xml" rel="alternate" title="Primary Feed" type="application/rss+xml" />
    <link href="/stylesheets/screen.css" media="screen" rel="Stylesheet" title="paperplanes" type="text/css" />
    <link rel="stylesheet" type="text/css" href="/stylesheets/mobile.css" media="handheld, only screen and (max-device-width: 960px)" />
  </head>
  
  <body id="www-paperplanes-de">
    <div id="head">
      <div id="header-content">
        <a href="/">
          <img src="/images/paperplane.png" id="paperplane">
        </a>
        <div id="about">
          <h1 class="default">Hi, I'm Mathias Meyer, nice to meet you!</h1>
          <h1 class="mobile">Hi, I'm <a href="https://twitter.com/roidrage">Mathias Meyer</a>, I'm the CEO at <a href="https://travis-ci.com">Travis CI</a></h1>
          <p style="color: white" class="about-sub-title default">
            I'm the CEO at <a href="http://travis-ci.com">Travis CI</a>. I like coffee, <a href="https://twitter.com/roidrage">Twitter</a> and <a href="mailto:meyer@paperplanes.de">email</a>.
          </p>
        </div>
      </div>
    </div>

    <div id="box">
      <div id="content">
        <div id="articles">

  <article>
    <div class="item">
      <div class="item_details">
        <h3><a href="/2013/10/18/the-smallest-distributed-system.html">The Smallest Distributed System</a></h3>
        <h4><a href="/2013/10/18/the-smallest-distributed-system.html" title="The Smallest Distributed System">18 October 2013</a> by <a href="http://twitter.com/roidrage">Mathias Meyer</a></h4>
      </div>
      <div class="item_content">
        <p><a href="https://travis-ci.org">Travis CI</a> started out as an idea, an ideal even. Before
its inception, there was a distinct lack of continuous integration systems
available for the open source community.</p>

<p>With GitHub on the rise as a collaboration platform for open source, what was
missing was a service to continuously test contributions and ensure that an open
source project is in a healthy state.</p>

<p>It started out in early 2011, and gained a bit of a follower-ship rather
quickly. By Summer 2011, we were doing 700 builds per day. All that was running
off a single build server. Travis CI integrated well with GitHub, which is to
this day still its main platform.</p>

<p>It didn&#39;t exactly break any new ground on the field of continuous integration,
but rather refined some existing concepts and added some new ideas. One was to
be able to look at your build logs streaming in while your tests are running, in
near real time.</p>

<p>On top of that, it allowed configuring the build by way of a file that&#39;s part of
your source code, the .travis.yml rather than a complex user interface.</p>

<p><img src="http://s3itch.paperplanes.de/The_Smallest_Distributed_System.004.jpg_20131018_153228.jpg" alt=""></p>

<p>The architecture started out very simple. A web component was responsible for
making builds and projects visually accessible, but also accepting webhook
notifications from GitHub, whenever a new commit was made to one of the projects
using it.</p>

<p>Another component, called hub, was responsible for processing new commits,
turning them into builds, and for processing the result data from jobs as they
ran and finished.</p>

<p>Both of them interacted with a single PostgreSQL database.</p>

<p>A third set of processes handled the build jobs themselves, executing a couple
of commands on a bunch of VirtualBox instances.</p>

<p><img src="http://s3itch.paperplanes.de/The_Smallest_Distributed_System.005.jpg_20131018_153658.jpg" alt=""></p>

<p>Under the hood, hub was slightly more complex than the rest of the system. It
interacted with RabbitMQ, where it processed the build logs as they come in.
Logs were streamed in chunks from the processes handling the build jobs.</p>

<p>Hub updated the database with the logs and build results, and it forwarded them
to Pusher. Using Pusher Travis CI could update the user interface as builds
started and as they finished.</p>

<p>This architecture took us into 2012, when we did 7000 builds per day. We saw
increasing traction in the open source community, and we grew to 11 supported
languages, among them PHP, Python, Perl, Java and Erlang.</p>

<p>With gaining traction, Travis CI became more and more of an integral service for
open source projects. Unfortunately, the system itself was never really built
with monitoring in mind.</p>

<p><img src="http://s3itch.paperplanes.de/The_Smallest_Distributed_System.008.jpg_20131018_153750.jpg" alt=""></p>

<p>It used to be people from the community who notified us that things weren&#39;t
working, that build jobs got stuck, that messages weren&#39;t being processed.</p>

<p>That was pretty embarrassing. Our first challenge was to add monitoring, metrics
and logging to a system that was slowly moving from a hobby project to an
important and also a commercial platform, as we were preparing the <a href="https://travis-ci.com">launch of
our productized version of Travis CI</a>.</p>

<p>Being notified by our customers that things weren&#39;t working was and still is to
this day my biggest nightmare, and we had to work hard to get all the right
metrics and monitoring in place for us to notice when things were breaking.</p>

<p>It was impossible for us to reason about what is happening in our little
distributed system without having any metrics or aggregated logging in place. By
all definitions, Travis CI was already a distributed system at this stage.</p>

<p>Adding metrics and logging was a very gradual learning experience, but in the
end, it gives us essential insight into what our system is doing, both in pretty
graphs and in logged words.</p>

<p>This was a big improvement for us, and it&#39;s an important takeaway. Visibility is
a key requirement for running a distributed system in production.</p>

<p>When you build it, think about how to monitor it.</p>

<p>Making this as easy as possible will help shape your code to run better in
production, not just to pass the tests.</p>

<p>The crux is that, with more monitoring, you gain not only more insight, you
suddenly find problems you haven&#39;t thought about or seen before. With great
visibility comes great responsibility. We now needed to embrace that we&#39;re more
aware of failures in our system, and that we need to actively work on decreasing
the risk of them affecting our system.</p>

<p><img src="http://s3itch.paperplanes.de/The_Smallest_Distributed_System.011.jpg_20131018_153850.jpg" alt=""></p>

<p>About a year ago, we saw the then current architecture breaking at the seams.
Hub in particular turned into a major concern, as it was laden with too many
responsibilities. It handled new commits, it processed and forwarded build logs,
it synchronized user data with GitHub, it notified users of broken and
successful builds. It talked to quite a bunch of external APIs along the way,
all in one process.</p>

<p>It just kept on growing, but it was impossible to scale. Hub could only run as a
single process and was therefore our biggest point of failure.</p>

<p>The GitHub API is an interesting example here. We&#39;re a heavy consumer of their
API, we rely on it heavily for builds to run. We pull build configuration via
the GitHub API, we update the build status for commits, we synchronize user data
with it.</p>

<p>Historically, when one of these failed, hub would just drop what it was working
on, moving on to the next. When the GitHub API was down, we lost a lot of
builds.</p>

<p>We put a lot of trust in the API, and we still do, but in the end, it&#39;s a
resource that is out of our hands. It&#39;s a resource that&#39;s separated from ours,
run by a different team, on a different network, with its own breaking points.</p>

<p>We just didn&#39;t treat it as such. We treated it as a friend who we can always
trust to answer our calls.</p>

<p>We were wrong.</p>

<p>A year ago, something changed in the API, unannounced. It was an undocumented
feature we relied on heavily. It was just disabled for the time being as it
caused problems on the other end.</p>

<p>On our end, all hell broke loose. For a simple reason too. We treated the GitHub
API as our friend, we patiently waited for it to answer our calls. We waited a
long time, for every single new commit that came in. We waited minutes for every
one of them.</p>

<p>Our timeouts were much too gracious. On top of that, when the calls finally
timed out, we&#39;d drop the build because we ran into an error. It was a long night
trying to isolate the problem.</p>

<p>Small things, when coming together at the right time, can break a system
spectacularly.</p>

<p><img src="http://s3itch.paperplanes.de/The_Smallest_Distributed_System.014.jpg_20131018_154118.jpg" alt=""></p>

<p>We started isolating the API calls, adding much shorter timeouts. To make sure
we don&#39;t drop builds because of temporary outages on GitHub&#39;s end, we added
retries as well. To make sure we handle extended outages better, we added
exponential backoff to the retries. With every retry, more time passes until the
next one.</p>

<p>External APIs beyond your control need to be treated as something that can fail
anytime. While you shouldn&#39;t try to isolate yourself from their failures, you
need to decide how you handle them.</p>

<p>How to handle every single failure scenario is a business decision. Can we
survive when we drop one build? Sure, it&#39;s not the end of the world. Should we
just drop hundreds of builds because of a problem outside of our control? We
shouldn&#39;t, because if anything, those builds matter to our customers.</p>

<p>Travis CI was built as a well-intentioned fellow. It assumed only the best and
just thought everything was working correctly all the time.</p>

<p>Unfortunately that&#39;s not the case. Everything can plunge into chaos, at any time,
and our code wasn&#39;t ready for it. We did quite a bit of work, and we still are,
to improve this situation, to improve how our own code handles failures in
external APIs, even in other components of our infrastructure.</p>

<p><img src="http://s3itch.paperplanes.de/The_Smallest_Distributed_System.017.jpg_20131018_154320.jpg" alt=""></p>

<p>Coming back to our friend, the hub, the tasks it did were easy to break out, so
we split out lots of smaller apps, one by one. Every app had its own purpose and
a pretty defined set of responsibilities.</p>

<p>Isolating the responsibilities allowed us to scale these processes out much
easier. Most of them were pretty straight-forward to break out.</p>

<p>We now had processes handling new commits, handling build notifications and
processing build logs.</p>

<p>Suddenly, we had another problem.</p>

<p>While our applications were now separate, they all shared a single dependency
called travis-core, which contains pretty much all of the business logic for all
parts of Travis CI. It&#39;s a <a href="https://en.wikipedia.org/wiki/Big_ball_of_mud">big ball of
mud</a>.</p>

<p><img src="http://s3itch.paperplanes.de/The_Smallest_Distributed_System.019.jpg_20131018_154454.jpg" alt=""></p>

<p>This single dependency meant that touching any part of the code could
potentially have implications on applications the code might not even be related
to. Our applications were split up by their responsibilities, but our code was
not.</p>

<p>We&#39;re still paying a price for architectural decisions made early on. Even a
simple dependency on one common piece of code can turn into a problem, as you
add more functionality, as you change code.</p>

<p>To make sure that code still works properly in all applications, we need to
regularly deploy all of them when updates are made to travis-core.</p>

<p>Responsibility doesn&#39;t just mean you need to separate concerns in code. They
need to be physically separated too.</p>

<p>Complex dependencies affect deployments which in turn affects your ability to
ship new code, new features.</p>

<p>We&#39;re slowly moving towards small dependencies, truly isolating every
application&#39;s responsibilities in code. Thankfully, the code itself is already
pretty isolated, making the transition easier for us.</p>

<p>One application in particular is worth a deeper look here, as it was our biggest
challenge to scale out.</p>

<p><img src="http://s3itch.paperplanes.de/The_Smallest_Distributed_System.022.jpg_20131018_154705.jpg" alt=""></p>

<p>Logs has two very simple responsibilities. When a chunk of a build log comes in
via the message queue, update a row in the database with it, then forward it to
Pusher for live user interface updates.</p>

<p>Log chunks are streamed from lots of different processes at the same time and
used to be processed by a single process, up to 100 messages per second.</p>

<p><img src="http://s3itch.paperplanes.de/The_Smallest_Distributed_System.023.jpg_20131018_154930.jpg" alt=""></p>

<p>While that was pretty okay for a single process it also meant that it&#39;s hard for
us to handle sudden bursts of log messages, and that this process would be a big
impediment for our growth, for scaling out.</p>

<p>The problem was that everything in Travis CI relied on these messages to be
processed in the order in which they were put on the message queue.</p>

<p>Updating a single log chunk in the database meant updating a single row which
contained the entire log as a single column. Updating the log in the user
interface simply meant appending to the end of the DOM.</p>

<p>To fix this particular problem, we had a lot of code to change.</p>

<p>But first we needed to figure out what would be a better solution, one that
would allow us to easily scale out log processing.</p>

<p>Instead of relying on the implicit order of messages as they were put on the
message queue, we decided to make the order an attribute of the message&#39;s
itself.</p>

<p><img src="http://s3itch.paperplanes.de/The_Smallest_Distributed_System.025.jpg_20131018_155010.jpg" alt=""></p>

<p>This is heavily inspired by Leslie Lamport&#39;s paper <a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/pubs.html#time-clocks">&quot;Time, Clocks, and the
Ordering of Events in a Distributed
System&quot;</a>
from 1978.</p>

<p>In the paper, Lamport describes the use of an incrementing clock to retain an
ordering of events in a distributed system. When a message is sent, the sender
increments the clock before it&#39;s forwarded to another receiver.</p>

<p>We could simplify on that idea, as one log chunk is always coming from just one
sender. That process can take care of always incrementing the clock, making it
easy to assemble a log from its chunks later on.</p>

<p>All that needs to be done is do sort the chunks by the clock.</p>

<p>The hard part was changing the relevant bits of the system to allow writing
small chunks to the database, aggregating them into full logs after the job was
done.</p>

<p>But it also directly affected our user interface. It would now have to deal with
messages arriving out of order. The work here was a bit more involved, but it
simplified a lot of other parts of our code in turn.</p>

<p>On the surface, it may not seem like a simplification. But relying on ordering
where you don&#39;t need to is an inherent source of implicit complexity. </p>

<p>We&#39;re now a lot less dependent on how messages are routed, how they&#39;re
transmitted, because we now we can restore their order at any time.</p>

<p>We had to change quite a bit of code, because that code assumed ordering where
there&#39;s actually chaos. In a distributed system, events can arrive out of order
at any time. We just had to make sure we can put the pieces back together later.</p>

<p>You can read more about how we solved this <a href="http://about.travis-ci.org/blog/2013-08-08-solving-the-puzzle-of-scalable-log-processing/">particular scaling issue on our
blog</a>.</p>

<p>Fast-forward to 2013, and we&#39;re running 45000 build jobs per day. We&#39;re still
paying for design decisions that were made early on, but we&#39;re slowly untangling
things.</p>

<p><img src="http://s3itch.paperplanes.de/The_Smallest_Distributed_System.028.jpg_20131018_155040.jpg" alt=""></p>

<p>We still have one flaw that we need to address. All of our components still
share the same database. This has the natural consequence that they&#39;ll all fail
should the database go down, as it did just last week.</p>

<p>But it also means that the number of log writes we do (currently up to 300 per
second) affects the read performance for our API, causing slower reads for our
customers and users when they&#39;re browsing through the user interface.</p>

<p>As we&#39;re heading towards another magnitude in the number of builds, our next
challenge is likely going to be to scale out our data storage.</p>

<p>Travis CI is not exactly a small distributed system anymore, now running off
more than 500 build servers. The problems we&#39;re tackling on solving are still on
a pretty small scale. But even on that scale, you can run into interesting
challenges, where simple approaches work much better than complex ones.</p>

      </div>
      <div class="item_meta">
        <span class="item_tags">
          Tags: 
          
        </span>
      </div>
    </div>
  </article>

  <article>
    <div class="item">
      <div class="item_details">
        <h3><a href="/2013/8/13/deploying-your-jekyll-blog-to-s3-with-travis-ci.html">Deploying your Jekyll Site to S3 with Travis CI</a></h3>
        <h4><a href="/2013/8/13/deploying-your-jekyll-blog-to-s3-with-travis-ci.html" title="Deploying your Jekyll Site to S3 with Travis CI">13 August 2013</a> by <a href="http://twitter.com/roidrage">Mathias Meyer</a></h4>
      </div>
      <div class="item_content">
        <p>I admit it, I got stung by the urge of not having to maintain my own servers
anymore. For simple websites like this one, it&#39;s become a dreadful task.</p>

<p>This site is built using <a href="http://jekyllrb.com">Jekyll</a>, I started using it very
early on, even had my custom fork with some fixes and improvements. That part
started to get annoying.  My fork was very outdated, and I really just had a
couple of patches around that, in the meantime and in one way or the other, had
made it into Jekyll&#39;s releases.</p>

<p>So I decided to take the plunge and stop hosting this site myself and put it on
<a href="https://aws.amazon.com/s3">S3</a>. Why Mathias, you&#39;ll ask, why didn&#39;t you just
use GitHub Pages?</p>

<p>I have some custom plugins and custom patches in place, for instance to create
pages for single tags, and I didn&#39;t want to rip out all that. GitHub Pages
unfortunately doesn&#39;t allow you to install custom plugin, which is of course
understandable from a security perspective.
<a href="http://wordpress.com">wordpress.com</a> imposes the same restriction on you.</p>

<p>Another option is to deploy generated content to GitHub Pages by adding and
committing to git, but I haven&#39;t been happy with the increasing delays imposed
by the CDN in front of GitHub Pages lately.</p>

<p>So I turned to my old friend S3. It&#39;s gotten some neat features to simplify
using it as a hosting platform over the years, like root document support,
redirects, built-in content delivery network, lots of good stuff.</p>

<h3>Local host is local most?</h3>

<p>One options is to deploy directly from my own machine, but that&#39;d deploy all
the local changes I have (and I do have some occasionally), so it&#39;d never really
be a clean slate.</p>

<p><a href="http://travis-ci.org">Travis CI</a> to the rescue! This little feat also makes me
eat my own dog food, working on Travis CI myself.</p>

<p>Even with a Jekyll site, deploying the content follows the simple pattern of
installing dependencies and building the site.</p>

<p>With the code always coming from GitHub, I could be sure that the deployed state
would always correspond to what&#39;s currently checked in.</p>

<p>First order of business: activate the repository on <a href="http://travis-ci.org">Travis
CI</a>, login required.</p>

<p><img src="http://s3itch.paperplanes.de/traviscihook_20130813_105140.jpg" width="550"/></p>

<p>Go to your profile, and turn on the repository. This enables the service hook on
GitHub, and all subsequent commits will be sent to Travis CI for instant
building.</p>

<h3>Continuous Synchronization</h3>

<p>To deploy your site, you&#39;ll need a few libraries, most importantly, jekyll.
There&#39;s a nice little tool called
<a href="https://github.com/laurilehmijoki/s3_website">&quot;s3_website&quot;</a>, which synchronizes
a static site with S3. It even supports Jekyll and its static content in the
<code>_site</code> directory, neat!</p>

<p>Here&#39;s the Gemfile:</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">source &quot;https://rubygems.org&quot;

gem &#39;jekyll&#39;
gem &#39;s3_website&#39;
gem &#39;redcarpet&#39;
</code></pre></div>
<p>I&#39;ve added Redcarpet too, as it&#39;s currently the best Markdown engine available
for Ruby. It&#39;s been serving me well for the <a href="http://riakhandbook.com">Riak Handbook</a> too!</p>

<p><code>s3_website</code> needs a configuration file. You can create a default using
<code>s3_website cfg create</code>.</p>

<p>Here&#39;s the <a href="https://github.com/roidrage/paperplanes/blob/master/s3_website.yml">one I use to deploy this very
site</a>:</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">s3_id: &lt;%= ENV[&#39;S3_ACCESS_KEY_ID&#39;] %&gt;
s3_secret: &lt;%= ENV[&#39;S3_SECRET_KEY&#39;] %&gt;
s3_bucket: www.paperplanes.de

max_age:
  &quot;public/*&quot;: 6000
  &quot;*&quot;: 300

s3_endpoint: us-east-1
s3_reduced_redundancy: true

concurrency_level: 100
</code></pre></div>
<p>Note that the bucket needs to exist before you&#39;re getting started. I recommend
naming it after your website, based on the domain people use to access it.</p>

<p>As you can then configure CNAME entries for site that points it to the bucket,
and S3 does automatic resolution of the right bucket for you.</p>

<h3>The build and deployment process</h3>

<p>The .travis.yml required to tell Travis CI is succinct and simple:</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">language: ruby
rvm:
  - 1.9.3
script: bundle exec jekyll build
install: bundle install
after_success: bundle exec s3_website push --headless
</code></pre></div>
<p>The language used is Ruby, and we&#39;re sticking to 1.9.3.</p>

<p>The <code>script</code> section defines the command to run to build the code, in this case
<code>jekyll build</code> which builds the site and generates the static result in the
<code>_site</code> folder.</p>

<p>The <code>install</code> section is customized to prevent Bundler from running with the
<code>--deployment</code> flag, which is the default for projects with a Gemfile.lock.
Unfortunately it leads to strange errors generating the site.</p>

<p>The last and most important bit is the code that ships the site to S3.
<code>s3_website push</code> synchronizes the static output with S3, only uploading files
that have changed. This last bit is very neat, as it keeps the whole sync
process nice and short.</p>

<h3>Secure credentials</h3>

<p>The last puzzle piece is the question of how to get those S3 credentials
securely into Travis CI without exposing them to the public.</p>

<p>The access keys are defined as environment variables in the configuration, so we
just need a way to have them set up for our build on Travis CI.</p>

<p>For this bit, you need the <code>travis</code> gem installed. Best to add it to your
Gemfile. Travis CI has a feature called <a href="http://about.travis-ci.org/docs/user/build-configuration/#Secure-environment-variables">&quot;secure environment
variables&quot;</a>,
that allow you to encrypt sensitive data in your .travis.yml to avoid exposing
it to the public.</p>

<p>I&#39;d recommend creating a separate user on Amazon IAM for this purpose, with
restricted permissions only for the bucket you&#39;re deploying too. You&#39;ll need
most permissions available, including listing, fetching and deleting objects.</p>

<p>When you have the credentials, run the following commands in your project&#39;s
directory. The <code>travis</code> tool will automatically look up the encryption keys for
you on Travis CI.</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">$ travis encrypt S3_ACCESS_KEY_ID=ASASK... --add env.global
$ travis encrypt S3_SECRET_KEY=sshhhhh... --add env.global
</code></pre></div>
<p>These commands add the environment variables to your .travis.yml. Add the file
to git, and push the changes to GitHub.</p>

<p>Now watch your blog be automatically deployed while also making sure the content
builds correctly.</p>

<h3>Fin</h3>

<p>This article about deploying your Jekyll site to S3 with Travis CI was
automatically deployed to S3 with Travis CI. Did that just blow your mind?</p>

      </div>
      <div class="item_meta">
        <span class="item_tags">
          Tags: 
          
        </span>
      </div>
    </div>
  </article>

  <article>
    <div class="item">
      <div class="item_details">
        <h3><a href="/2013/8/9/what-is-automation.html">What is Automation?</a></h3>
        <h4><a href="/2013/8/9/what-is-automation.html" title="What is Automation?">09 August 2013</a> by <a href="http://twitter.com/roidrage">Mathias Meyer</a></h4>
      </div>
      <div class="item_content">
        <p>In our circles, automation is most frequently associated with infrastructure,
tools like Puppet and Chef, automating away your server provisioning and
deployments.</p>

<p>While this is an important part and by now ingrained in our culture thanks to
DevOps, it&#39;s by far the only part that defines automation in distributed
systems.</p>

<p>The biggest challenge of a distributed system is to run reliably, without much
human intervention, and entirely on its own.</p>

<p>It has to handle network partition, increased error conditions in part of the
system, deal with scalability issues, provide good performance throughout.</p>

<p>It has to deal with failure constantly.</p>

<p>We&#39;d like the impact of handling failures to be as little as possible, to
require as little human involvement as possible. The distributed system ideally
takes care of itself as much as possible.</p>

<p>We&#39;ve introduced barriers, bulkheads, circuit breakers, exponential back-offs,
all to make sure our distributed system doesn&#39;t kill itself. Unfortunately they
have a tendency to do that, especially under conditions with increased load or
increased failure rates.</p>

<p>This is the part where automation gets interesting and incredibly hard. This is
the part where you decide how a system should behave when certain conditions
occur in production. This is the part where you consider how you want humans to
notice and interpret the unusual conditions and how they should respond to it.</p>

<p>According to <a href="http://amzn.to/18fCnpX">&quot;Thinking in Systems&quot;</a>, complex systems
are based on feedback loops. They continue to feed activity in the system, and
changes in the feedback loop affect the outcome with every pass.</p>

<p>In distributed system, the most common feedback loop you&#39;ll find is a
<a href="http://www.systems-thinking.org/theWay/sre/re.htm">reinforcing feedback loop</a>.
Instead of just feeding the loop, it increases or decreases its output to get to
the desired outcome in a way that affects other components in the system.</p>

<p>Slow request times caused requests to pile up and continue to hammer the system
more and more. Back-offs and retries hit the system more than once, causing an
increase in overall request volume. Slow disks affect all parts that read from
them, causing delays throughout. I&#39;m sure you can think of lots of other
scenarios.</p>

<p>As we learned, <a href="http://www.paperplanes.de/2012/7/10/on-resilience-in-automated-systems-failures-and-human-factor.html">humans make poor
monitors</a>
for automated systems, so they need to be as tightly integrated into the process
  as possible, allowing highest visibility into what the system is doing.</p>

<p>While the system should be able to defend itself against any production issues,
it shouldn&#39;t cut out the operator, quite the opposite. The operator should be an
integral part of the process of running it.</p>

<p>But everything the system is doing, what it has been tasked by its engineers to
do under specific circumstances, is automation.</p>

<p>What is automation?</p>

<p>What follows is the simplest definition I could think of that applies best to
the system we build. I&#39;d love to hear your take on it.</p>

<p><strong>Automation is everything a complex system decides on doing in response to
ever-changing and emerging conditions.</strong></p>

      </div>
      <div class="item_meta">
        <span class="item_tags">
          Tags: 
          
        </span>
      </div>
    </div>
  </article>

</div>

<div class="pagination">
  
    
      <a href="/page16" class="previous">Newer Posts</a>
    
  
  
    <a href="/page18" class="next">Older Posts</a>
  
</div>

       
        <div id="footer">
          <div id="footer_text">
            <a href="/archives.html">Archives</a>, <a href="http://www.paperplanes.de/rss.xml" title="Full-text RSS feed">RSS Feed</a>, &copy; 2007-2014 Mathias Meyer <a href="/imprint.html">Imprint</a>
          </div>
        </div>
      </div>
    </div>

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-46305173-1', 'paperplanes.de');
      ga('send', 'pageview');

    </script>
  </body>
  <script src="//my.hellobar.com/7db1d1ae6111ae95568efbbf8e6a1ee953ad854f.js" type="text/javascript" charset="utf-8" async="async"></script>
</html>
